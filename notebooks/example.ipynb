{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook\n",
    "builds a graph-based embedding for a small number of random handwritten digits with multidimensional scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: run this before import to recompile c++ ops\n",
    "```\n",
    "!rm -r ../lib/cpp/temp/\n",
    "!rm -r ../lib/cpp/build/\n",
    "!rm -r ../lib/cpp/_bindings.so\n",
    "!rm -r ../lib/cpp/bindings.py\n",
    "!rm -r ../lib/cpp/lib_wrap.c\n",
    "```\n",
    "\n",
    "\n",
    "##### Get the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import lib # builds c++ binaries with swig\n",
    "from lib.utils.distance_helper import compute_original_pairwise_distance, compute_pca_pairwise_distance, mds_pairwise_distance\n",
    "from lib import data_loader as data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024 # 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_loader.get_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = compute_original_pairwise_distance(X, simple=True, num_samples=len(X))\n",
    "dist_pca = compute_pca_pairwise_distance(X, simple=True, num_samples=len(X))\n",
    "dist_mds = mds_pairwise_distance(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA:\t 0.00504\n",
      "MDS:\t 0.00506\n"
     ]
    }
   ],
   "source": [
    "print(\"PCA:\\t %.5f\" % np.mean(np.square(distances - dist_pca)))\n",
    "print(\"MDS:\\t %.5f\" % np.mean(np.square(distances - dist_mds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build initial graph\n",
    "\n",
    "We initialize prodige with a full graph initialized with distances between nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.task.compression import make_graph_from_vectors\n",
    "emb = make_graph_from_vectors(\n",
    "    X, knn_edges=100, max_length=10, n_jobs=-1, soft=True, directed=False, verbose=True\n",
    ")\n",
    "\n",
    "opt = torch.optim.SparseAdam(emb.parameters(), lr=0.01)\n",
    "\n",
    "loss_history, reg_history = [], []\n",
    "\n",
    "# uncomment to deliberately mess with weights for testing purposes\n",
    "# emb.edge_weight_logits.reset_parameters()\n",
    "# emb.edge_adjacency_logits.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop\n",
    "\n",
    "Here we minimize the MDS loss function\n",
    "$$L = 1/N \\sum_{i, j} (d_{orig}(x_i, x_j) - d_G(v_i, v_j))^2$$\n",
    "\n",
    "* $d_{orig}(x_i, x_j)$ is the original distance between two vectors in $X$\n",
    "* $d_G(v_i, v_j)$ is the learned graph distance between corresponding vertices in graph $G$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i in tqdm(range(len(loss_history), 650)): # 1000\n",
    "    ii = torch.randint(0, len(X), [batch_size])\n",
    "    jj = torch.randint(0, len(X), [batch_size])\n",
    "\n",
    "    pred = emb(ii, jj)\n",
    "    distances_ref = torch.as_tensor(distances[ii, jj], dtype=torch.float32)\n",
    "    \n",
    "    reconstruction_mse = F.mse_loss(pred['target_distances'], distances_ref)\n",
    "    \n",
    "    if len(loss_history) < 325: # 5000\n",
    "        regularizer = emb.compute_l0_prior_penalty(batch_size=4096)\n",
    "    else:\n",
    "        regularizer = emb.compute_hierarchical_prior_penalty(nonzero_rate=0.05, batch_size=4096)\n",
    "\n",
    "    lambd = min(1, len(loss_history) / 5000.) * 10.0\n",
    "    loss = reconstruction_mse - pred['logp_target_paths'].mean() + lambd * regularizer\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loss_history.append(reconstruction_mse.item())\n",
    "    reg_history.append(regularizer.item())\n",
    "    \n",
    "    if len(loss_history) % 10 == 0: # 50\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[15, 4])\n",
    "        plt.subplot(1, 3, 1);\n",
    "        plt.title('reconstruction mse, mean = %0.5f' % np.mean(loss_history[-100:])); plt.grid()\n",
    "        plt.plot(loss_history)\n",
    "        \n",
    "        plt.subplot(1, 3, 2);\n",
    "        plt.title('regularizer, mean = %0.5f' % np.mean(reg_history[-100:])); plt.grid()\n",
    "        plt.plot(reg_history)\n",
    "\n",
    "        plt.subplot(1, 3, 3);\n",
    "        probs = torch.sigmoid(emb.edge_adjacency_logits).data.numpy().flatten()\n",
    "        nnz_rate = np.mean(probs > 0.5)\n",
    "        plt.title('P(edge), nonzero rate = %.5f' % nnz_rate); plt.grid();\n",
    "        plt.hist(probs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_ours = emb.compute_pairwise_distances()\n",
    "print(\"PRODIGE: %.5f\" % np.mean(np.square(distances - distances_ours)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh import io\n",
    "from bokeh.plotting import figure, output_file, save\n",
    "output_file('mnist_graph.html')\n",
    "emb.default_distance.data[...] = 100\n",
    "p = lib.visualize_embeddings(emb, vertex_labels=y, edge_probability_threshold=0.5, deterministic=True,\n",
    "                         vertex_alpha=0.8, cmap=lambda x: plt.get_cmap('rainbow')(x))\n",
    "save(p, 'mnist_graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(filename=\"graph.html\")\n",
    "# re-run several times to get different tsne optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
